{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f75a8d99",
   "metadata": {},
   "source": [
    "# Rachel Ney-Grimm Midterm Project - Sakila Datamart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5eae76",
   "metadata": {},
   "source": [
    "First, I imported libraries and set up the connection to the MySQL Server and MongoDB, where I will retrieve data from. Then I added the functions for getting and setting database data, and I created the Sakila_2 database, the destination of the ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03908c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import os\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "import datetime\n",
    "import pymongo\n",
    "\n",
    "#connection setup\n",
    "host_name = \"localhost\"\n",
    "host_ip = \"127.0.0.1\"\n",
    "port = \"3306\"\n",
    "user_id = \"root\"\n",
    "pwd = \"Passw0rd123\"\n",
    "\n",
    "src_dbname = \"sakila\"\n",
    "dst_dbname = \"sakila_2\"\n",
    "\n",
    "#database function definitions\n",
    "def get_mongo_dataframe(connect_str, db_name, collection, query):\n",
    "    '''Create a connection to MongoDB'''\n",
    "    client = pymongo.MongoClient(connect_str)\n",
    "    \n",
    "    '''Query MongoDB, and fill a python list with documents to create a DataFrame'''\n",
    "    db = client[db_name]\n",
    "    dframe = pd.DataFrame(list(db[collection].find(query)))\n",
    "    dframe.drop(['_id'], axis=1, inplace=True)\n",
    "    client.close()\n",
    "    return dframe\n",
    "\n",
    "def get_sql_dataframe(user_id, pwd, host_name, db_name, sql_query):\n",
    "    conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}/{db_name}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    dframe = pd.read_sql(sql_query, connection);\n",
    "    connection.close()\n",
    "    return dframe\n",
    "\n",
    "def set_dataframe(user_id, pwd, host_name, db_name, df, table_name, pk_column, db_operation):\n",
    "    conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}/{db_name}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    \n",
    "    if db_operation == \"insert\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='replace')\n",
    "        sqlEngine.execute(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({pk_column});\")\n",
    "            \n",
    "    elif db_operation == \"update\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='append')\n",
    "    \n",
    "    connection.close()\n",
    "    \n",
    "#creating new database    \n",
    "conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}\"\n",
    "sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "\n",
    "sqlEngine.execute(f\"DROP DATABASE IF EXISTS `{dst_dbname}`;\")\n",
    "sqlEngine.execute(f\"CREATE DATABASE `{dst_dbname}`;\")\n",
    "sqlEngine.execute(f\"USE {dst_dbname};\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1062d5b2",
   "metadata": {},
   "source": [
    "## Customer Dimension Table\n",
    "I created a 'customer' dimension table from data originating at the MySQL sakila database. I transformed this data by renaming what will be table's key, and dropping the columns that aren't needed in the new schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the data from the source with a sql select statement into a dataframe\n",
    "sql_customers = \"SELECT * FROM sakila.customers;\"\n",
    "df_customers = get_sql_dataframe(user_id, pwd, host_name, src_dbname, sql_customers)\n",
    "df_customers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc851c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename id column to fit data warehouse standard, drop undesired columns\n",
    "df_customers.rename(columns={\"id\":\"customer_key\"}, inplace=True)\n",
    "df_customers.drop(['store_id'], axis=1, inplace=True)\n",
    "df_customers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19a08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7300d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for adding address to the customer dimension tables\n",
    "#bring address, city, and country tables in as dataframes\n",
    "#drop columns that are empty ('address2') or contain binary large objects that won't be conducive to analysis (location')\n",
    "sql_address = \"SELECT * FROM sakila.address;\"\n",
    "df_address = get_sql_dataframe(user_id, pwd, host_name, src_dbname, sql_address)\n",
    "df_address.drop(['location', 'address2', 'last_update'], axis=1, inplace=True)\n",
    "\n",
    "sql_city = \"SELECT city_id , city , country_id FROM sakila.city;\"\n",
    "df_city = get_sql_dataframe(user_id, pwd, host_name, src_dbname, sql_city)\n",
    "\n",
    "sql_country = \"SELECT country_id, country FROM sakila.country;\"\n",
    "df_country = get_sql_dataframe(user_id, pwd, host_name, src_dbname, sql_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532ca076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge country into city\n",
    "df_city= pd.merge(df_city, df_country, on='country_id', how='left')#or inner\n",
    "#merge city into address\n",
    "df_address= pd.merge(df_address, df_city, on='city_id', how='left')\n",
    "#merge address into the customer table - end goal\n",
    "df_customer= pd.merge(df_customer, df_address, on='address_id', how='left')\n",
    "    #the address information is now attached in the dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608400f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1b9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095a347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b894c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load into data warehouse\n",
    "#dataframe -> tables\n",
    "db_operation = \"insert\"\n",
    "tables = [('dim_customers', df_customers, 'customer_key')]\n",
    "for table_name, dataframe, primary_key in tables:\n",
    "    set_dataframe(user_id, pwd, host_name, dst_dbname, dataframe, table_name, primary_key, db_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e3f78",
   "metadata": {},
   "source": [
    "### Film Dimension Table\n",
    "Data on the film's language is retrieved from the file system. The film data itself is retrieved from MongoDB. Simple transformations (dropping unnecessary or duplicate columns, renaming the key) are made. The film dataframe is then merged with the language dataframe on the shared language_id column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9909cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the sakila language data from the file system\n",
    "#will later merge to become part of the film dimension table\n",
    "data_file = os.path.join(os.getcwd(), 'sakila_language_data.csv')\n",
    "df_language = pd.read_csv(data_file, header=0, index_col=0\n",
    "df_language.drop(['last_update'], axis=1, inplace=True)\n",
    "df_language.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908d0331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sakila film data from mongodb\n",
    "query = {}\n",
    "collection = \"film\"\n",
    "df_film = get_mongo_dataframe(conn_str['local'], src_dbname, collection, query)#select everthing from film use to make a df\n",
    "df_film.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c6bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform initial transormations on film data\n",
    "#drop data we are not interested in for analyzing the business processes\n",
    "df_film.drop(['description','original_language_id'], axis=1, inplace=True)\n",
    "df_film.rename(columns={\"id\":\"film_key\"}, inplace=True) #dont want id to be called just id\n",
    "df_film.head(2) #id column now film_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad55a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge language and film on language id (left) using pandas\n",
    "df_film = pd.merge(df_film, df_language, on='language_id', how='left')#or inner\n",
    "df_film.rename(columns={\"name\":\"film_language\"}) #rename the column containing the language of the film to be more intuitive\n",
    "df_film.drop(['language_id'], axis=1, inplace=True)\n",
    "df_film.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46564394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the film dataframe into newly created film table in datawarehouse, \n",
    "dataframe = df_film\n",
    "table_name = 'dim_film'\n",
    "primary_key = 'film_key'\n",
    "db_operation = \"insert\"\n",
    "\n",
    "set_dataframe(mysql_uid, mysql_pwd, dst_dbname, dataframe, table_name, primary_key, db_operation)\n",
    "\n",
    "#validate that it was created and loaded successfully\n",
    "sql_film = \"SELECT * FROM sakila_2.dim_film;\"\n",
    "df_dim_film = get_sql_dataframe(mysql_uid, mysql_pwd, dst_dbname, sql_film)\n",
    "df_dim_film.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e84470",
   "metadata": {},
   "source": [
    "## Fact Table\n",
    "The fact table is based primarily off of the rental process that is modeled by the sakila database. It also includes data pertaining to the payments for the rentals to provide more quantitative information on the transaction. To create this fact table, sakila's rental table and payment table were joined using the pandas merge method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987150ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_rental = \"SELECT * FROM sakila.rental;\"\n",
    "df_rental = get_sql_dataframe(user_id, pwd, host_name, src_dbname, sql_rental)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac8ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_payment = \"SELECT * FROM sakila.payment;\"\n",
    "df_payment = get_sql_dataframe(user_id, pwd, host_name, src_dbname, sql_payment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rental = pd.merge(df_rental, df_payment, on='rental_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de4461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f096664d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
